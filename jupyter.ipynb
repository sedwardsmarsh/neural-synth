{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Union\n",
    "from tensorflow.python.keras import callbacks, Sequential, layers, Model\n",
    "import glob\n",
    "import os\n",
    "from scipy.io.wavfile import read\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "from numpy import ndarray\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import midiDriver\n",
    "import audioRecorder\n",
    "\n",
    "\n",
    "CHECKPOINT_FILEPATH = './checkpoints'\n",
    "\n",
    "\n",
    "def esr(signal_a, signal_b) -> float:\n",
    "    '''Returns the Error-to-Signal Ratio.\n",
    "\n",
    "    Keyword arguments:\n",
    "    signal_a -- the groundtruth signal\n",
    "    signal_b -- the predicted signal\n",
    "    '''\n",
    "    power = 2.0\n",
    "    numerator = np.sum(np.power(np.subtract(signal_a, signal_b), power))\n",
    "    denominator = np.sum(np.power(signal_a, power))\n",
    "    return np.divide(numerator, denominator)\n",
    "    \n",
    "\n",
    "def normalize(array: Union[List,ndarray], scale_max: int=1, scale_min: int=0) -> List:\n",
    "    '''Returns a normalized array.\n",
    "    \n",
    "    Keyword arguments:\n",
    "    array -- array to normalize\n",
    "    scale_max -- maximum value to scale between\n",
    "    scale_min -- minimum value to scale between\n",
    "    '''\n",
    "    scaler = MinMaxScaler(feature_range=(scale_min, scale_max))\n",
    "    return scaler.fit_transform(array)\n",
    "\n",
    "\n",
    "def partition_dataset(data_path: str='data/simple_dataset/01/*', train_perc: float=0.8) -> Tuple[List, List, List, List]: \n",
    "    '''Partition into train, train_labels & test, test_labels datasets.\n",
    "\n",
    "    Keyword arguments:\n",
    "    data_path -- where the data lives\n",
    "    '''\n",
    "    assert train_perc < 1, 'train_perc must be less than 1'\n",
    "    data_paths = glob.glob(data_path)\n",
    "    split_idx = int(0.8 * len(data_paths))\n",
    "    train_paths, test_paths = data_paths[:split_idx], data_paths[split_idx:]\n",
    "    train_data, test_data = [], []\n",
    "    train_labels, test_labels = [], []\n",
    "    for file in train_paths:\n",
    "        _, data = read(file)\n",
    "        train_data.append(data)\n",
    "        train_labels.append(os.path.basename(file).split('.')[0])\n",
    "    for file in test_paths:\n",
    "        _, data = read(file)\n",
    "        test_data.append(data)\n",
    "        test_labels.append(os.path.basename(file).split('.')[0])\n",
    "    return train_data, train_labels, test_data, test_labels\n",
    "\n",
    "\n",
    "def make_model(input_shape: Tuple[int, int], num_output_nodes: int=3) -> Sequential:\n",
    "    '''Return a model.\n",
    "    \n",
    "    Keyword arguments:\n",
    "    input_shape -- dimension of input features\n",
    "    num_output_nodes -- number of nodes of the output layer\n",
    "    '''\n",
    "    model = Sequential([\n",
    "        InputLayer(input_shape=input_shape),\n",
    "        # units = 8 is a value taken from https://arxiv.org/pdf/2009.02833.pdf\n",
    "        Conv1D(filters=32, kernel_size=3, activation='relu'), \n",
    "        Conv1D(filters=32, kernel_size=3, activation='relu'),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        # Flatten(),\n",
    "        # Dense(10),\n",
    "        GRU(units=8, return_sequences=True), \n",
    "        Dense(num_output_nodes)\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load features and labels and convert them to lists\n",
    "CSV_PATH = './data/preprocessed/'\n",
    "CSV_FILENAME = 'datasetFeat&LabelNorm2022-05-13 14:20:12.csv'\n",
    "\n",
    "df = pd.read_csv(CSV_PATH + CSV_FILENAME)\n",
    "X = [eval(feature_str) for feature_str in df['features']]\n",
    "X = np.asarray(X)\n",
    "\n",
    "y = [eval(label_str) for label_str in df['labels']]\n",
    "y = np.asarray(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train.shape=(600, 43) x_val.shape=(200, 43) x_test.shape=(200, 43)\n"
     ]
    }
   ],
   "source": [
    "# partition dataset into 60% train, 20% val, 20% test\n",
    "split_seed: int = 42\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=split_seed)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size=0.25, random_state=split_seed)\n",
    "print(f'{x_train.shape=} {x_val.shape=} {x_test.shape=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# architect model\n",
    "num_labels = y.shape[1]\n",
    "input_shape = X.shape[1]\n",
    "\n",
    "# librosa features head\n",
    "libosa_inputs = layers.Input(shape=input_shape, name='librosa features')\n",
    "x = layers.Dense(128, activation='relu')(libosa_inputs)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "x = layers.Dense(16, activation='relu')(x)\n",
    "block_1_output = layers.Dropout(0.5)(x)\n",
    "# block 2\n",
    "x = layers.Dense(16, activation='relu', name='2')(block_1_output)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "block_2_output = layers.add([x, block_1_output])\n",
    "# block 3\n",
    "x = layers.Dense(16, activation='relu', name='3')(block_2_output)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "block_3_output = layers.add([x, block_2_output])\n",
    "# block 4\n",
    "x = layers.Dense(16, activation='relu', name='4')(block_3_output)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "block_4_output = layers.add([x, block_3_output])\n",
    "# block 5\n",
    "x = layers.Dense(16, activation='relu', name='5')(block_4_output)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "block_5_output = layers.add([x, block_4_output])\n",
    "# block 6\n",
    "x = layers.Dense(16, activation='relu', name='6')(block_5_output)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "block_6_output = layers.add([x, block_5_output])\n",
    "# block 7\n",
    "x = layers.Dense(16, activation='relu', name='7')(block_6_output)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "block_7_output = layers.add([x, block_6_output])\n",
    "# output block\n",
    "output_block = layers.Dense(num_labels, name='output')(block_7_output)\n",
    "# model definition\n",
    "model = Model(inputs=[libosa_inputs], outputs=[output_block], name='functional')\n",
    "\n",
    "# visualize model \n",
    "from keras.utils.all_utils import plot_model\n",
    "# plot_model(model, show_shapes=True, rankdir='TB', to_file='model.png', dpi=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-13 14:25:10.504365: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 2s 44ms/step - loss: 1.5115 - val_loss: 0.2966\n",
      "Epoch 2/400\n",
      " 4/10 [===========>..................] - ETA: 0s - loss: 0.5021"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-13 14:25:11.540363: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 21ms/step - loss: 0.4606 - val_loss: 0.2721\n",
      "Epoch 3/400\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 0.3298 - val_loss: 0.2477\n",
      "Epoch 4/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.2576 - val_loss: 0.2179\n",
      "Epoch 5/400\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 0.2229 - val_loss: 0.1929\n",
      "Epoch 6/400\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 0.1860 - val_loss: 0.1639\n",
      "Epoch 7/400\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 0.1599 - val_loss: 0.1412\n",
      "Epoch 8/400\n",
      "10/10 [==============================] - 0s 17ms/step - loss: 0.1374 - val_loss: 0.1232\n",
      "Epoch 9/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.1218 - val_loss: 0.1098\n",
      "Epoch 10/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.1077 - val_loss: 0.0989\n",
      "Epoch 11/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.1003 - val_loss: 0.0899\n",
      "Epoch 12/400\n",
      "10/10 [==============================] - 0s 16ms/step - loss: 0.0932 - val_loss: 0.0870\n",
      "Epoch 13/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0868 - val_loss: 0.0819\n",
      "Epoch 14/400\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 0.0833 - val_loss: 0.0789\n",
      "Epoch 15/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0777 - val_loss: 0.0758\n",
      "Epoch 16/400\n",
      "10/10 [==============================] - 0s 16ms/step - loss: 0.0734 - val_loss: 0.0754\n",
      "Epoch 17/400\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 0.0735 - val_loss: 0.0747\n",
      "Epoch 18/400\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 0.0703 - val_loss: 0.0744\n",
      "Epoch 19/400\n",
      "10/10 [==============================] - 0s 16ms/step - loss: 0.0678 - val_loss: 0.0731\n",
      "Epoch 20/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0667 - val_loss: 0.0755\n",
      "Epoch 21/400\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 0.0670 - val_loss: 0.0738\n",
      "Epoch 22/400\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 0.0634 - val_loss: 0.0734\n",
      "Epoch 23/400\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 0.0645 - val_loss: 0.0746\n",
      "Epoch 24/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0638 - val_loss: 0.0739\n",
      "Epoch 25/400\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 0.0633 - val_loss: 0.0741\n",
      "Epoch 26/400\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 0.0618 - val_loss: 0.0722\n",
      "Epoch 27/400\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 0.0621 - val_loss: 0.0739\n",
      "Epoch 28/400\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 0.0612 - val_loss: 0.0736\n",
      "Epoch 29/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0612 - val_loss: 0.0741\n",
      "Epoch 30/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0607 - val_loss: 0.0720\n",
      "Epoch 31/400\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 0.0602 - val_loss: 0.0732\n",
      "Epoch 32/400\n",
      "10/10 [==============================] - 0s 17ms/step - loss: 0.0593 - val_loss: 0.0709\n",
      "Epoch 33/400\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 0.0576 - val_loss: 0.0726\n",
      "Epoch 34/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0582 - val_loss: 0.0733\n",
      "Epoch 35/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0566 - val_loss: 0.0735\n",
      "Epoch 36/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0569 - val_loss: 0.0731\n",
      "Epoch 37/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0575 - val_loss: 0.0704\n",
      "Epoch 38/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0566 - val_loss: 0.0702\n",
      "Epoch 39/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0570 - val_loss: 0.0730\n",
      "Epoch 40/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0561 - val_loss: 0.0713\n",
      "Epoch 41/400\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 0.0541 - val_loss: 0.0701\n",
      "Epoch 42/400\n",
      "10/10 [==============================] - 0s 17ms/step - loss: 0.0543 - val_loss: 0.0705\n",
      "Epoch 43/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0562 - val_loss: 0.0702\n",
      "Epoch 44/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0537 - val_loss: 0.0704\n",
      "Epoch 45/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0527 - val_loss: 0.0681\n",
      "Epoch 46/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0524 - val_loss: 0.0699\n",
      "Epoch 47/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0533 - val_loss: 0.0688\n",
      "Epoch 48/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0541 - val_loss: 0.0661\n",
      "Epoch 49/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0516 - val_loss: 0.0681\n",
      "Epoch 50/400\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 0.0517 - val_loss: 0.0662\n",
      "Epoch 51/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0499 - val_loss: 0.0681\n",
      "Epoch 52/400\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 0.0503 - val_loss: 0.0697\n",
      "Epoch 53/400\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 0.0510 - val_loss: 0.0694\n",
      "Epoch 54/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0510 - val_loss: 0.0652\n",
      "Epoch 55/400\n",
      "10/10 [==============================] - 0s 17ms/step - loss: 0.0507 - val_loss: 0.0657\n",
      "Epoch 56/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0516 - val_loss: 0.0668\n",
      "Epoch 57/400\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 0.0503 - val_loss: 0.0661\n",
      "Epoch 58/400\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 0.0479 - val_loss: 0.0629\n",
      "Epoch 59/400\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 0.0488 - val_loss: 0.0662\n",
      "Epoch 60/400\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 0.0487 - val_loss: 0.0665\n",
      "Epoch 61/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0459 - val_loss: 0.0656\n",
      "Epoch 62/400\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 0.0481 - val_loss: 0.0647\n",
      "Epoch 63/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0470 - val_loss: 0.0628\n",
      "Epoch 64/400\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 0.0479 - val_loss: 0.0656\n",
      "Epoch 65/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0480 - val_loss: 0.0670\n",
      "Epoch 66/400\n",
      "10/10 [==============================] - 0s 17ms/step - loss: 0.0465 - val_loss: 0.0646\n",
      "Epoch 67/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0453 - val_loss: 0.0651\n",
      "Epoch 68/400\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 0.0445 - val_loss: 0.0641\n",
      "Epoch 69/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0453 - val_loss: 0.0638\n",
      "Epoch 70/400\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 0.0450 - val_loss: 0.0615\n",
      "Epoch 71/400\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 0.0431 - val_loss: 0.0606\n",
      "Epoch 72/400\n",
      "10/10 [==============================] - 0s 16ms/step - loss: 0.0430 - val_loss: 0.0578\n",
      "Epoch 73/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0434 - val_loss: 0.0542\n",
      "Epoch 74/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0412 - val_loss: 0.0575\n",
      "Epoch 75/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0409 - val_loss: 0.0613\n",
      "Epoch 76/400\n",
      "10/10 [==============================] - 0s 16ms/step - loss: 0.0444 - val_loss: 0.0621\n",
      "Epoch 77/400\n",
      "10/10 [==============================] - 0s 17ms/step - loss: 0.0415 - val_loss: 0.0571\n",
      "Epoch 78/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0406 - val_loss: 0.0604\n",
      "Epoch 79/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0404 - val_loss: 0.0546\n",
      "Epoch 80/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0404 - val_loss: 0.0587\n",
      "Epoch 81/400\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 0.0390 - val_loss: 0.0552\n",
      "Epoch 82/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0397 - val_loss: 0.0579\n",
      "Epoch 83/400\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 0.0395 - val_loss: 0.0548\n",
      "Epoch 84/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0400 - val_loss: 0.0601\n",
      "Epoch 85/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0393 - val_loss: 0.0554\n",
      "Epoch 86/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0387 - val_loss: 0.0570\n",
      "Epoch 87/400\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 0.0395 - val_loss: 0.0487\n",
      "Epoch 88/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0398 - val_loss: 0.0584\n",
      "Epoch 89/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0377 - val_loss: 0.0567\n",
      "Epoch 90/400\n",
      "10/10 [==============================] - 0s 17ms/step - loss: 0.0398 - val_loss: 0.0626\n",
      "Epoch 91/400\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 0.0387 - val_loss: 0.0531\n",
      "Epoch 92/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0372 - val_loss: 0.0479\n",
      "Epoch 93/400\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 0.0371 - val_loss: 0.0485\n",
      "Epoch 94/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0346 - val_loss: 0.0509\n",
      "Epoch 95/400\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 0.0349 - val_loss: 0.0542\n",
      "Epoch 96/400\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 0.0357 - val_loss: 0.0476\n",
      "Epoch 97/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0353 - val_loss: 0.0531\n",
      "Epoch 98/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0356 - val_loss: 0.0537\n",
      "Epoch 99/400\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 0.0369 - val_loss: 0.0516\n",
      "Epoch 100/400\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 0.0355 - val_loss: 0.0477\n",
      "Epoch 101/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0350 - val_loss: 0.0506\n",
      "Epoch 102/400\n",
      "10/10 [==============================] - 0s 16ms/step - loss: 0.0348 - val_loss: 0.0491\n",
      "Epoch 103/400\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 0.0342 - val_loss: 0.0478\n",
      "Epoch 104/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0339 - val_loss: 0.0488\n",
      "Epoch 105/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0342 - val_loss: 0.0523\n",
      "Epoch 106/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0337 - val_loss: 0.0442\n",
      "Epoch 107/400\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 0.0336 - val_loss: 0.0462\n",
      "Epoch 108/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0315 - val_loss: 0.0467\n",
      "Epoch 109/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0330 - val_loss: 0.0513\n",
      "Epoch 110/400\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 0.0328 - val_loss: 0.0478\n",
      "Epoch 111/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0325 - val_loss: 0.0416\n",
      "Epoch 112/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0311 - val_loss: 0.0458\n",
      "Epoch 113/400\n",
      "10/10 [==============================] - 0s 17ms/step - loss: 0.0330 - val_loss: 0.0471\n",
      "Epoch 114/400\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 0.0317 - val_loss: 0.0470\n",
      "Epoch 115/400\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 0.0323 - val_loss: 0.0475\n",
      "Epoch 116/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0327 - val_loss: 0.0459\n",
      "Epoch 117/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0309 - val_loss: 0.0491\n",
      "Epoch 118/400\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 0.0305 - val_loss: 0.0493\n",
      "Epoch 119/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0302 - val_loss: 0.0467\n",
      "Epoch 120/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0320 - val_loss: 0.0429\n",
      "Epoch 121/400\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 0.0313 - val_loss: 0.0422\n",
      "Epoch 122/400\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 0.0303 - val_loss: 0.0539\n",
      "Epoch 123/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0318 - val_loss: 0.0479\n",
      "Epoch 124/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0295 - val_loss: 0.0443\n",
      "Epoch 125/400\n",
      "10/10 [==============================] - 0s 17ms/step - loss: 0.0289 - val_loss: 0.0409\n",
      "Epoch 126/400\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 0.0300 - val_loss: 0.0438\n",
      "Epoch 127/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0289 - val_loss: 0.0489\n",
      "Epoch 128/400\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 0.0304 - val_loss: 0.0448\n",
      "Epoch 129/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0274 - val_loss: 0.0427\n",
      "Epoch 130/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0296 - val_loss: 0.0417\n",
      "Epoch 131/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0298 - val_loss: 0.0492\n",
      "Epoch 132/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0276 - val_loss: 0.0460\n",
      "Epoch 133/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0279 - val_loss: 0.0405\n",
      "Epoch 134/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0290 - val_loss: 0.0393\n",
      "Epoch 135/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0293 - val_loss: 0.0426\n",
      "Epoch 136/400\n",
      "10/10 [==============================] - 0s 16ms/step - loss: 0.0291 - val_loss: 0.0439\n",
      "Epoch 137/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0281 - val_loss: 0.0421\n",
      "Epoch 138/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0289 - val_loss: 0.0441\n",
      "Epoch 139/400\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 0.0271 - val_loss: 0.0424\n",
      "Epoch 140/400\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 0.0281 - val_loss: 0.0411\n",
      "Epoch 141/400\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 0.0276 - val_loss: 0.0394\n",
      "Epoch 142/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0278 - val_loss: 0.0375\n",
      "Epoch 143/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0284 - val_loss: 0.0420\n",
      "Epoch 144/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0291 - val_loss: 0.0407\n",
      "Epoch 145/400\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 0.0271 - val_loss: 0.0422\n",
      "Epoch 146/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0256 - val_loss: 0.0393\n",
      "Epoch 147/400\n",
      "10/10 [==============================] - 0s 17ms/step - loss: 0.0270 - val_loss: 0.0385\n",
      "Epoch 148/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0269 - val_loss: 0.0374\n",
      "Epoch 149/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0268 - val_loss: 0.0381\n",
      "Epoch 150/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0264 - val_loss: 0.0370\n",
      "Epoch 151/400\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 0.0256 - val_loss: 0.0389\n",
      "Epoch 152/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0271 - val_loss: 0.0413\n",
      "Epoch 153/400\n",
      "10/10 [==============================] - 0s 16ms/step - loss: 0.0264 - val_loss: 0.0397\n",
      "Epoch 154/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0266 - val_loss: 0.0445\n",
      "Epoch 155/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0256 - val_loss: 0.0388\n",
      "Epoch 156/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0257 - val_loss: 0.0367\n",
      "Epoch 157/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0264 - val_loss: 0.0357\n",
      "Epoch 158/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0269 - val_loss: 0.0369\n",
      "Epoch 159/400\n",
      "10/10 [==============================] - 0s 16ms/step - loss: 0.0259 - val_loss: 0.0386\n",
      "Epoch 160/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0262 - val_loss: 0.0426\n",
      "Epoch 161/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0262 - val_loss: 0.0404\n",
      "Epoch 162/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0247 - val_loss: 0.0379\n",
      "Epoch 163/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0275 - val_loss: 0.0431\n",
      "Epoch 164/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0258 - val_loss: 0.0359\n",
      "Epoch 165/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0250 - val_loss: 0.0356\n",
      "Epoch 166/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0257 - val_loss: 0.0421\n",
      "Epoch 167/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0255 - val_loss: 0.0397\n",
      "Epoch 168/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0246 - val_loss: 0.0336\n",
      "Epoch 169/400\n",
      "10/10 [==============================] - 0s 17ms/step - loss: 0.0247 - val_loss: 0.0385\n",
      "Epoch 170/400\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 0.0247 - val_loss: 0.0329\n",
      "Epoch 171/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0246 - val_loss: 0.0373\n",
      "Epoch 172/400\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 0.0246 - val_loss: 0.0336\n",
      "Epoch 173/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0231 - val_loss: 0.0381\n",
      "Epoch 174/400\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 0.0238 - val_loss: 0.0380\n",
      "Epoch 175/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0245 - val_loss: 0.0366\n",
      "Epoch 176/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0239 - val_loss: 0.0356\n",
      "Epoch 177/400\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 0.0236 - val_loss: 0.0373\n",
      "Epoch 178/400\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 0.0235 - val_loss: 0.0388\n",
      "Epoch 179/400\n",
      "10/10 [==============================] - 0s 16ms/step - loss: 0.0244 - val_loss: 0.0430\n",
      "Epoch 180/400\n",
      "10/10 [==============================] - 0s 16ms/step - loss: 0.0249 - val_loss: 0.0321\n",
      "Epoch 181/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0223 - val_loss: 0.0394\n",
      "Epoch 182/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0248 - val_loss: 0.0397\n",
      "Epoch 183/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0239 - val_loss: 0.0354\n",
      "Epoch 184/400\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 0.0241 - val_loss: 0.0339\n",
      "Epoch 185/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0238 - val_loss: 0.0430\n",
      "Epoch 186/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0245 - val_loss: 0.0367\n",
      "Epoch 187/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0232 - val_loss: 0.0447\n",
      "Epoch 188/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0224 - val_loss: 0.0408\n",
      "Epoch 189/400\n",
      "10/10 [==============================] - 0s 16ms/step - loss: 0.0232 - val_loss: 0.0359\n",
      "Epoch 190/400\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 0.0221 - val_loss: 0.0422\n",
      "Epoch 191/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0229 - val_loss: 0.0331\n",
      "Epoch 192/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0241 - val_loss: 0.0371\n",
      "Epoch 193/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0211 - val_loss: 0.0337\n",
      "Epoch 194/400\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 0.0228 - val_loss: 0.0358\n",
      "Epoch 195/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0230 - val_loss: 0.0372\n",
      "Epoch 196/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0222 - val_loss: 0.0357\n",
      "Epoch 197/400\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 0.0238 - val_loss: 0.0394\n",
      "Epoch 198/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0221 - val_loss: 0.0369\n",
      "Epoch 199/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0217 - val_loss: 0.0415\n",
      "Epoch 200/400\n",
      "10/10 [==============================] - 0s 16ms/step - loss: 0.0227 - val_loss: 0.0379\n",
      "Epoch 201/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0220 - val_loss: 0.0361\n",
      "Epoch 202/400\n",
      "10/10 [==============================] - 0s 23ms/step - loss: 0.0219 - val_loss: 0.0400\n",
      "Epoch 203/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0238 - val_loss: 0.0344\n",
      "Epoch 204/400\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 0.0226 - val_loss: 0.0317\n",
      "Epoch 205/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0215 - val_loss: 0.0341\n",
      "Epoch 206/400\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 0.0225 - val_loss: 0.0333\n",
      "Epoch 207/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0204 - val_loss: 0.0340\n",
      "Epoch 208/400\n",
      "10/10 [==============================] - 0s 16ms/step - loss: 0.0229 - val_loss: 0.0408\n",
      "Epoch 209/400\n",
      "10/10 [==============================] - 0s 16ms/step - loss: 0.0210 - val_loss: 0.0386\n",
      "Epoch 210/400\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 0.0213 - val_loss: 0.0316\n",
      "Epoch 211/400\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 0.0220 - val_loss: 0.0382\n",
      "Epoch 212/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0221 - val_loss: 0.0373\n",
      "Epoch 213/400\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 0.0212 - val_loss: 0.0373\n",
      "Epoch 214/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0208 - val_loss: 0.0469\n",
      "Epoch 215/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0219 - val_loss: 0.0375\n",
      "Epoch 216/400\n",
      "10/10 [==============================] - 0s 16ms/step - loss: 0.0208 - val_loss: 0.0337\n",
      "Epoch 217/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0216 - val_loss: 0.0401\n",
      "Epoch 218/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0212 - val_loss: 0.0397\n",
      "Epoch 219/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0207 - val_loss: 0.0354\n",
      "Epoch 220/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0209 - val_loss: 0.0391\n",
      "Epoch 221/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0205 - val_loss: 0.0398\n",
      "Epoch 222/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0204 - val_loss: 0.0401\n",
      "Epoch 223/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0212 - val_loss: 0.0425\n",
      "Epoch 224/400\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 0.0221 - val_loss: 0.0375\n",
      "Epoch 225/400\n",
      "10/10 [==============================] - 0s 16ms/step - loss: 0.0200 - val_loss: 0.0451\n",
      "Epoch 226/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0194 - val_loss: 0.0370\n",
      "Epoch 227/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0200 - val_loss: 0.0385\n",
      "Epoch 228/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0215 - val_loss: 0.0417\n",
      "Epoch 229/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0198 - val_loss: 0.0351\n",
      "Epoch 230/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0207 - val_loss: 0.0383\n",
      "Epoch 231/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0206 - val_loss: 0.0375\n",
      "Epoch 232/400\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 0.0193 - val_loss: 0.0342\n",
      "Epoch 233/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0215 - val_loss: 0.0383\n",
      "Epoch 234/400\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 0.0197 - val_loss: 0.0381\n",
      "Epoch 235/400\n",
      "10/10 [==============================] - 0s 16ms/step - loss: 0.0206 - val_loss: 0.0409\n",
      "Epoch 236/400\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 0.0207 - val_loss: 0.0394\n",
      "Epoch 237/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0195 - val_loss: 0.0397\n",
      "Epoch 238/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0201 - val_loss: 0.0382\n",
      "Epoch 239/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0197 - val_loss: 0.0345\n",
      "Epoch 240/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0201 - val_loss: 0.0362\n",
      "Epoch 241/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0194 - val_loss: 0.0404\n",
      "Epoch 242/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0201 - val_loss: 0.0380\n",
      "Epoch 243/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0212 - val_loss: 0.0377\n",
      "Epoch 244/400\n",
      "10/10 [==============================] - 0s 16ms/step - loss: 0.0192 - val_loss: 0.0413\n",
      "Epoch 245/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0204 - val_loss: 0.0436\n",
      "Epoch 246/400\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 0.0188 - val_loss: 0.0354\n",
      "Epoch 247/400\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 0.0188 - val_loss: 0.0401\n",
      "Epoch 248/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0205 - val_loss: 0.0364\n",
      "Epoch 249/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0204 - val_loss: 0.0376\n",
      "Epoch 250/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0193 - val_loss: 0.0364\n",
      "Epoch 251/400\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 0.0194 - val_loss: 0.0418\n",
      "Epoch 252/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0204 - val_loss: 0.0350\n",
      "Epoch 253/400\n",
      "10/10 [==============================] - 0s 16ms/step - loss: 0.0197 - val_loss: 0.0377\n",
      "Epoch 254/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0200 - val_loss: 0.0338\n",
      "Epoch 255/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0193 - val_loss: 0.0445\n",
      "Epoch 256/400\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 0.0195 - val_loss: 0.0374\n",
      "Epoch 257/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0203 - val_loss: 0.0342\n",
      "Epoch 258/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0182 - val_loss: 0.0318\n",
      "Epoch 259/400\n",
      "10/10 [==============================] - 0s 16ms/step - loss: 0.0194 - val_loss: 0.0385\n",
      "Epoch 260/400\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 0.0190 - val_loss: 0.0381\n",
      "Epoch 261/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0192 - val_loss: 0.0306\n",
      "Epoch 262/400\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 0.0197 - val_loss: 0.0363\n",
      "Epoch 263/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0195 - val_loss: 0.0403\n",
      "Epoch 264/400\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 0.0183 - val_loss: 0.0448\n",
      "Epoch 265/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0195 - val_loss: 0.0366\n",
      "Epoch 266/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0199 - val_loss: 0.0415\n",
      "Epoch 267/400\n",
      "10/10 [==============================] - 0s 16ms/step - loss: 0.0194 - val_loss: 0.0423\n",
      "Epoch 268/400\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 0.0191 - val_loss: 0.0398\n",
      "Epoch 269/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0186 - val_loss: 0.0333\n",
      "Epoch 270/400\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 0.0196 - val_loss: 0.0419\n",
      "Epoch 271/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0181 - val_loss: 0.0429\n",
      "Epoch 272/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0192 - val_loss: 0.0332\n",
      "Epoch 273/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0179 - val_loss: 0.0410\n",
      "Epoch 274/400\n",
      "10/10 [==============================] - 0s 16ms/step - loss: 0.0184 - val_loss: 0.0418\n",
      "Epoch 275/400\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 0.0182 - val_loss: 0.0351\n",
      "Epoch 276/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0197 - val_loss: 0.0393\n",
      "Epoch 277/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0189 - val_loss: 0.0347\n",
      "Epoch 278/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0184 - val_loss: 0.0369\n",
      "Epoch 279/400\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 0.0194 - val_loss: 0.0402\n",
      "Epoch 280/400\n",
      "10/10 [==============================] - 0s 16ms/step - loss: 0.0181 - val_loss: 0.0395\n",
      "Epoch 281/400\n",
      "10/10 [==============================] - 0s 16ms/step - loss: 0.0183 - val_loss: 0.0386\n",
      "Epoch 282/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0189 - val_loss: 0.0377\n",
      "Epoch 283/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0182 - val_loss: 0.0423\n",
      "Epoch 284/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0178 - val_loss: 0.0377\n",
      "Epoch 285/400\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 0.0184 - val_loss: 0.0403\n",
      "Epoch 286/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0186 - val_loss: 0.0398\n",
      "Epoch 287/400\n",
      "10/10 [==============================] - 0s 16ms/step - loss: 0.0180 - val_loss: 0.0420\n",
      "Epoch 288/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0186 - val_loss: 0.0439\n",
      "Epoch 289/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0178 - val_loss: 0.0441\n",
      "Epoch 290/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0189 - val_loss: 0.0322\n",
      "Epoch 291/400\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 0.0172 - val_loss: 0.0434\n",
      "Epoch 292/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0171 - val_loss: 0.0350\n",
      "Epoch 293/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0182 - val_loss: 0.0363\n",
      "Epoch 294/400\n",
      "10/10 [==============================] - 0s 18ms/step - loss: 0.0184 - val_loss: 0.0360\n",
      "Epoch 295/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0180 - val_loss: 0.0392\n",
      "Epoch 296/400\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 0.0184 - val_loss: 0.0446\n",
      "Epoch 297/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0180 - val_loss: 0.0352\n",
      "Epoch 298/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0178 - val_loss: 0.0453\n",
      "Epoch 299/400\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 0.0186 - val_loss: 0.0405\n",
      "Epoch 300/400\n",
      "10/10 [==============================] - 0s 16ms/step - loss: 0.0168 - val_loss: 0.0328\n",
      "Epoch 301/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0175 - val_loss: 0.0388\n",
      "Epoch 302/400\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 0.0181 - val_loss: 0.0431\n",
      "Epoch 303/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0183 - val_loss: 0.0422\n",
      "Epoch 304/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0189 - val_loss: 0.0491\n",
      "Epoch 305/400\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 0.0191 - val_loss: 0.0484\n",
      "Epoch 306/400\n",
      "10/10 [==============================] - 0s 16ms/step - loss: 0.0176 - val_loss: 0.0417\n",
      "Epoch 307/400\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 0.0176 - val_loss: 0.0363\n",
      "Epoch 308/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0180 - val_loss: 0.0412\n",
      "Epoch 309/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0179 - val_loss: 0.0408\n",
      "Epoch 310/400\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 0.0180 - val_loss: 0.0484\n",
      "Epoch 311/400\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 0.0176 - val_loss: 0.0408\n",
      "Epoch 312/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0179 - val_loss: 0.0431\n",
      "Epoch 313/400\n",
      "10/10 [==============================] - 0s 16ms/step - loss: 0.0172 - val_loss: 0.0438\n",
      "Epoch 314/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0178 - val_loss: 0.0368\n",
      "Epoch 315/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0168 - val_loss: 0.0405\n",
      "Epoch 316/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0174 - val_loss: 0.0426\n",
      "Epoch 317/400\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 0.0169 - val_loss: 0.0390\n",
      "Epoch 318/400\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 0.0175 - val_loss: 0.0423\n",
      "Epoch 319/400\n",
      "10/10 [==============================] - 0s 16ms/step - loss: 0.0177 - val_loss: 0.0368\n",
      "Epoch 320/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0167 - val_loss: 0.0370\n",
      "Epoch 321/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0171 - val_loss: 0.0379\n",
      "Epoch 322/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0165 - val_loss: 0.0369\n",
      "Epoch 323/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0170 - val_loss: 0.0406\n",
      "Epoch 324/400\n",
      "10/10 [==============================] - 0s 17ms/step - loss: 0.0175 - val_loss: 0.0451\n",
      "Epoch 325/400\n",
      "10/10 [==============================] - 0s 16ms/step - loss: 0.0162 - val_loss: 0.0425\n",
      "Epoch 326/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0166 - val_loss: 0.0401\n",
      "Epoch 327/400\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 0.0166 - val_loss: 0.0377\n",
      "Epoch 328/400\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 0.0168 - val_loss: 0.0435\n",
      "Epoch 329/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0165 - val_loss: 0.0444\n",
      "Epoch 330/400\n",
      "10/10 [==============================] - 0s 17ms/step - loss: 0.0174 - val_loss: 0.0434\n",
      "Epoch 331/400\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 0.0166 - val_loss: 0.0449\n",
      "Epoch 332/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0173 - val_loss: 0.0379\n",
      "Epoch 333/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0164 - val_loss: 0.0377\n",
      "Epoch 334/400\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 0.0165 - val_loss: 0.0368\n",
      "Epoch 335/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0177 - val_loss: 0.0397\n",
      "Epoch 336/400\n",
      "10/10 [==============================] - 0s 16ms/step - loss: 0.0169 - val_loss: 0.0412\n",
      "Epoch 337/400\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 0.0162 - val_loss: 0.0401\n",
      "Epoch 338/400\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 0.0172 - val_loss: 0.0440\n",
      "Epoch 339/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0168 - val_loss: 0.0422\n",
      "Epoch 340/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0165 - val_loss: 0.0337\n",
      "Epoch 341/400\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 0.0179 - val_loss: 0.0483\n",
      "Epoch 342/400\n",
      "10/10 [==============================] - 0s 17ms/step - loss: 0.0170 - val_loss: 0.0433\n",
      "Epoch 343/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0166 - val_loss: 0.0410\n",
      "Epoch 344/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0162 - val_loss: 0.0455\n",
      "Epoch 345/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0173 - val_loss: 0.0412\n",
      "Epoch 346/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0169 - val_loss: 0.0424\n",
      "Epoch 347/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0170 - val_loss: 0.0388\n",
      "Epoch 348/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0165 - val_loss: 0.0427\n",
      "Epoch 349/400\n",
      "10/10 [==============================] - 0s 16ms/step - loss: 0.0166 - val_loss: 0.0503\n",
      "Epoch 350/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0164 - val_loss: 0.0363\n",
      "Epoch 351/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0158 - val_loss: 0.0481\n",
      "Epoch 352/400\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 0.0171 - val_loss: 0.0429\n",
      "Epoch 353/400\n",
      "10/10 [==============================] - 0s 17ms/step - loss: 0.0159 - val_loss: 0.0402\n",
      "Epoch 354/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0166 - val_loss: 0.0441\n",
      "Epoch 355/400\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 0.0164 - val_loss: 0.0418\n",
      "Epoch 356/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0169 - val_loss: 0.0436\n",
      "Epoch 357/400\n",
      "10/10 [==============================] - 0s 16ms/step - loss: 0.0160 - val_loss: 0.0481\n",
      "Epoch 358/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0163 - val_loss: 0.0479\n",
      "Epoch 359/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0165 - val_loss: 0.0417\n",
      "Epoch 360/400\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 0.0163 - val_loss: 0.0431\n",
      "Epoch 361/400\n",
      "10/10 [==============================] - 0s 16ms/step - loss: 0.0161 - val_loss: 0.0465\n",
      "Epoch 362/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0163 - val_loss: 0.0540\n",
      "Epoch 363/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0170 - val_loss: 0.0395\n",
      "Epoch 364/400\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 0.0162 - val_loss: 0.0355\n",
      "Epoch 365/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0160 - val_loss: 0.0415\n",
      "Epoch 366/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0167 - val_loss: 0.0426\n",
      "Epoch 367/400\n",
      "10/10 [==============================] - 0s 16ms/step - loss: 0.0158 - val_loss: 0.0413\n",
      "Epoch 368/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0165 - val_loss: 0.0443\n",
      "Epoch 369/400\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 0.0169 - val_loss: 0.0376\n",
      "Epoch 370/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0163 - val_loss: 0.0477\n",
      "Epoch 371/400\n",
      "10/10 [==============================] - 0s 16ms/step - loss: 0.0157 - val_loss: 0.0406\n",
      "Epoch 372/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0158 - val_loss: 0.0403\n",
      "Epoch 373/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0165 - val_loss: 0.0400\n",
      "Epoch 374/400\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 0.0158 - val_loss: 0.0415\n",
      "Epoch 375/400\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 0.0166 - val_loss: 0.0452\n",
      "Epoch 376/400\n",
      "10/10 [==============================] - 0s 16ms/step - loss: 0.0164 - val_loss: 0.0466\n",
      "Epoch 377/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0163 - val_loss: 0.0428\n",
      "Epoch 378/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0156 - val_loss: 0.0399\n",
      "Epoch 379/400\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 0.0169 - val_loss: 0.0373\n",
      "Epoch 380/400\n",
      "10/10 [==============================] - 0s 16ms/step - loss: 0.0161 - val_loss: 0.0407\n",
      "Epoch 381/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0164 - val_loss: 0.0434\n",
      "Epoch 382/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0165 - val_loss: 0.0442\n",
      "Epoch 383/400\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 0.0160 - val_loss: 0.0405\n",
      "Epoch 384/400\n",
      "10/10 [==============================] - 0s 16ms/step - loss: 0.0163 - val_loss: 0.0372\n",
      "Epoch 385/400\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 0.0161 - val_loss: 0.0405\n",
      "Epoch 386/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0157 - val_loss: 0.0435\n",
      "Epoch 387/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0162 - val_loss: 0.0446\n",
      "Epoch 388/400\n",
      "10/10 [==============================] - 0s 16ms/step - loss: 0.0163 - val_loss: 0.0423\n",
      "Epoch 389/400\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 0.0159 - val_loss: 0.0483\n",
      "Epoch 390/400\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 0.0162 - val_loss: 0.0421\n",
      "Epoch 391/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0158 - val_loss: 0.0402\n",
      "Epoch 392/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0155 - val_loss: 0.0414\n",
      "Epoch 393/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0151 - val_loss: 0.0368\n",
      "Epoch 394/400\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 0.0163 - val_loss: 0.0424\n",
      "Epoch 395/400\n",
      "10/10 [==============================] - 0s 16ms/step - loss: 0.0157 - val_loss: 0.0446\n",
      "Epoch 396/400\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 0.0149 - val_loss: 0.0368\n",
      "Epoch 397/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0152 - val_loss: 0.0462\n",
      "Epoch 398/400\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0172 - val_loss: 0.0463\n",
      "Epoch 399/400\n",
      "10/10 [==============================] - 0s 16ms/step - loss: 0.0159 - val_loss: 0.0395\n",
      "Epoch 400/400\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 0.0157 - val_loss: 0.0455\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "from tensorflow.python.keras.optimizer_v2.rmsprop import RMSProp\n",
    "PARAMS = {\n",
    "    'opt' : RMSProp(learning_rate=0.001), # 0.002:742, 0.003:612, 0.004:668, 0.006:605\n",
    "    'loss' : 'mse',\n",
    "    'bs' : 64,\n",
    "    'epochs' : 400, # 500 became stagnant around 220 with lr=0.001\n",
    "    'metrics' : []\n",
    "}\n",
    "model.compile(loss=PARAMS['loss'], metrics=PARAMS['metrics'], optimizer=PARAMS['opt'])\n",
    "\n",
    "# model_checkpoint_callback = callbacks.ModelCheckpoint(\n",
    "#     save_best_only=True, save_weights_only=False, \n",
    "#     filepath=CHECKPOINT_FILEPATH, monitor='val_loss', mode='auto')\n",
    "reduce_lr_callback = callbacks.ReduceLROnPlateau(patience=50, monitor='val_loss', factor=0.1, mode='min')\n",
    "\n",
    "history = model.fit(\n",
    "    x_train, y_train, batch_size=PARAMS['bs'], \n",
    "    epochs=PARAMS['epochs'], validation_data=(x_val, y_val), \n",
    "    verbose=1,) # callbacks=[reduce_lr_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Training Loss: 0.0455\n",
      "Final Testing Loss: 0.0455\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'lr'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/Users/sadedwar/code/fun/ga-synth/jupyter.ipynb Cell 6'\u001b[0m in \u001b[0;36m<cell line: 25>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sadedwar/code/fun/ga-synth/jupyter.ipynb#ch0000015?line=22'>23</a>\u001b[0m \u001b[39m#second plot\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sadedwar/code/fun/ga-synth/jupyter.ipynb#ch0000015?line=23'>24</a>\u001b[0m ax \u001b[39m=\u001b[39m fig\u001b[39m.\u001b[39madd_subplot(gs[\u001b[39m1\u001b[39m], sharex\u001b[39m=\u001b[39max)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/sadedwar/code/fun/ga-synth/jupyter.ipynb#ch0000015?line=24'>25</a>\u001b[0m ax\u001b[39m.\u001b[39mplot(history\u001b[39m.\u001b[39;49mhistory[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sadedwar/code/fun/ga-synth/jupyter.ipynb#ch0000015?line=25'>26</a>\u001b[0m ax\u001b[39m.\u001b[39mset_ylabel(\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39mlearning rate\u001b[39m\u001b[39m'\u001b[39m, size \u001b[39m=\u001b[39m\u001b[39m16\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sadedwar/code/fun/ga-synth/jupyter.ipynb#ch0000015?line=26'>27</a>\u001b[0m ax\u001b[39m.\u001b[39mget_yaxis()\u001b[39m.\u001b[39mset_label_coords(\u001b[39m-\u001b[39m\u001b[39m0.17\u001b[39m,\u001b[39m0.5\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'lr'"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"421.601pt\" height=\"465.958125pt\" viewBox=\"0 0 421.601 465.958125\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n <metadata>\n  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2022-05-13T14:26:08.910829</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.5.1, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 465.958125 \nL 421.601 465.958125 \nL 421.601 0 \nL 0 0 \nz\n\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 79.601 204.872727 \nL 414.401 204.872727 \nL 414.401 7.2 \nL 79.601 7.2 \nz\n\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path id=\"mf7db47d992\" d=\"M 0 0 \nL 0 3.5 \n\" style=\"stroke: #ffffff; stroke-width: 0.8\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#mf7db47d992\" x=\"94.819182\" y=\"204.872727\" style=\"fill: #ffffff; stroke: #ffffff; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <g style=\"fill: #ffffff\" transform=\"translate(91.637932 219.471165)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \nQ 1547 4250 1301 3770 \nQ 1056 3291 1056 2328 \nQ 1056 1369 1301 889 \nQ 1547 409 2034 409 \nQ 2525 409 2770 889 \nQ 3016 1369 3016 2328 \nQ 3016 3291 2770 3770 \nQ 2525 4250 2034 4250 \nz\nM 2034 4750 \nQ 2819 4750 3233 4129 \nQ 3647 3509 3647 2328 \nQ 3647 1150 3233 529 \nQ 2819 -91 2034 -91 \nQ 1250 -91 836 529 \nQ 422 1150 422 2328 \nQ 422 3509 836 4129 \nQ 1250 4750 2034 4750 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use xlink:href=\"#mf7db47d992\" x=\"132.959988\" y=\"204.872727\" style=\"fill: #ffffff; stroke: #ffffff; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 50 -->\n      <g style=\"fill: #ffffff\" transform=\"translate(126.597488 219.471165)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-35\" d=\"M 691 4666 \nL 3169 4666 \nL 3169 4134 \nL 1269 4134 \nL 1269 2991 \nQ 1406 3038 1543 3061 \nQ 1681 3084 1819 3084 \nQ 2600 3084 3056 2656 \nQ 3513 2228 3513 1497 \nQ 3513 744 3044 326 \nQ 2575 -91 1722 -91 \nQ 1428 -91 1123 -41 \nQ 819 9 494 109 \nL 494 744 \nQ 775 591 1075 516 \nQ 1375 441 1709 441 \nQ 2250 441 2565 725 \nQ 2881 1009 2881 1497 \nQ 2881 1984 2565 2268 \nQ 2250 2553 1709 2553 \nQ 1456 2553 1204 2497 \nQ 953 2441 691 2322 \nL 691 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-35\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use xlink:href=\"#mf7db47d992\" x=\"171.100795\" y=\"204.872727\" style=\"fill: #ffffff; stroke: #ffffff; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 100 -->\n      <g style=\"fill: #ffffff\" transform=\"translate(161.557045 219.471165)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-31\" d=\"M 794 531 \nL 1825 531 \nL 1825 4091 \nL 703 3866 \nL 703 4441 \nL 1819 4666 \nL 2450 4666 \nL 2450 531 \nL 3481 531 \nL 3481 0 \nL 794 0 \nL 794 531 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use xlink:href=\"#mf7db47d992\" x=\"209.241602\" y=\"204.872727\" style=\"fill: #ffffff; stroke: #ffffff; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 150 -->\n      <g style=\"fill: #ffffff\" transform=\"translate(199.697852 219.471165)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-35\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use xlink:href=\"#mf7db47d992\" x=\"247.382408\" y=\"204.872727\" style=\"fill: #ffffff; stroke: #ffffff; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 200 -->\n      <g style=\"fill: #ffffff\" transform=\"translate(237.838658 219.471165)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \nL 3431 531 \nL 3431 0 \nL 469 0 \nL 469 531 \nQ 828 903 1448 1529 \nQ 2069 2156 2228 2338 \nQ 2531 2678 2651 2914 \nQ 2772 3150 2772 3378 \nQ 2772 3750 2511 3984 \nQ 2250 4219 1831 4219 \nQ 1534 4219 1204 4116 \nQ 875 4013 500 3803 \nL 500 4441 \nQ 881 4594 1212 4672 \nQ 1544 4750 1819 4750 \nQ 2544 4750 2975 4387 \nQ 3406 4025 3406 3419 \nQ 3406 3131 3298 2873 \nQ 3191 2616 2906 2266 \nQ 2828 2175 2409 1742 \nQ 1991 1309 1228 531 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use xlink:href=\"#mf7db47d992\" x=\"285.523215\" y=\"204.872727\" style=\"fill: #ffffff; stroke: #ffffff; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 250 -->\n      <g style=\"fill: #ffffff\" transform=\"translate(275.979465 219.471165)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use xlink:href=\"#DejaVuSans-35\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_7\">\n      <g>\n       <use xlink:href=\"#mf7db47d992\" x=\"323.664021\" y=\"204.872727\" style=\"fill: #ffffff; stroke: #ffffff; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 300 -->\n      <g style=\"fill: #ffffff\" transform=\"translate(314.120271 219.471165)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-33\" d=\"M 2597 2516 \nQ 3050 2419 3304 2112 \nQ 3559 1806 3559 1356 \nQ 3559 666 3084 287 \nQ 2609 -91 1734 -91 \nQ 1441 -91 1130 -33 \nQ 819 25 488 141 \nL 488 750 \nQ 750 597 1062 519 \nQ 1375 441 1716 441 \nQ 2309 441 2620 675 \nQ 2931 909 2931 1356 \nQ 2931 1769 2642 2001 \nQ 2353 2234 1838 2234 \nL 1294 2234 \nL 1294 2753 \nL 1863 2753 \nQ 2328 2753 2575 2939 \nQ 2822 3125 2822 3475 \nQ 2822 3834 2567 4026 \nQ 2313 4219 1838 4219 \nQ 1578 4219 1281 4162 \nQ 984 4106 628 3988 \nL 628 4550 \nQ 988 4650 1302 4700 \nQ 1616 4750 1894 4750 \nQ 2613 4750 3031 4423 \nQ 3450 4097 3450 3541 \nQ 3450 3153 3228 2886 \nQ 3006 2619 2597 2516 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-33\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_8\">\n     <g id=\"line2d_8\">\n      <g>\n       <use xlink:href=\"#mf7db47d992\" x=\"361.804828\" y=\"204.872727\" style=\"fill: #ffffff; stroke: #ffffff; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 350 -->\n      <g style=\"fill: #ffffff\" transform=\"translate(352.261078 219.471165)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-33\"/>\n       <use xlink:href=\"#DejaVuSans-35\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_9\">\n     <g id=\"line2d_9\">\n      <g>\n       <use xlink:href=\"#mf7db47d992\" x=\"399.945634\" y=\"204.872727\" style=\"fill: #ffffff; stroke: #ffffff; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 400 -->\n      <g style=\"fill: #ffffff\" transform=\"translate(390.401884 219.471165)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-34\" d=\"M 2419 4116 \nL 825 1625 \nL 2419 1625 \nL 2419 4116 \nz\nM 2253 4666 \nL 3047 4666 \nL 3047 1625 \nL 3713 1625 \nL 3713 1100 \nL 3047 1100 \nL 3047 0 \nL 2419 0 \nL 2419 1100 \nL 313 1100 \nL 313 1709 \nL 2253 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-34\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_10\">\n      <defs>\n       <path id=\"m6ebf0f4420\" d=\"M 0 0 \nL -3.5 0 \n\" style=\"stroke: #ffffff; stroke-width: 0.8\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#m6ebf0f4420\" x=\"79.601\" y=\"121.90273\" style=\"fill: #ffffff; stroke: #ffffff; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- $\\mathdefault{10^{-1}}$ -->\n      <g style=\"fill: #ffffff\" transform=\"translate(49.101 125.701949)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-2212\" d=\"M 678 2272 \nL 4684 2272 \nL 4684 1741 \nL 678 1741 \nL 678 2272 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-31\" transform=\"translate(0 0.684375)\"/>\n       <use xlink:href=\"#DejaVuSans-30\" transform=\"translate(63.623047 0.684375)\"/>\n       <use xlink:href=\"#DejaVuSans-2212\" transform=\"translate(128.203125 38.965625)scale(0.7)\"/>\n       <use xlink:href=\"#DejaVuSans-31\" transform=\"translate(186.855469 38.965625)scale(0.7)\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_11\">\n      <g>\n       <use xlink:href=\"#m6ebf0f4420\" x=\"79.601\" y=\"32.26695\" style=\"fill: #ffffff; stroke: #ffffff; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- $\\mathdefault{10^{0}}$ -->\n      <g style=\"fill: #ffffff\" transform=\"translate(55.001 36.066168)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\" transform=\"translate(0 0.765625)\"/>\n       <use xlink:href=\"#DejaVuSans-30\" transform=\"translate(63.623047 0.765625)\"/>\n       <use xlink:href=\"#DejaVuSans-30\" transform=\"translate(128.203125 39.046875)scale(0.7)\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_12\">\n      <defs>\n       <path id=\"mca119c7480\" d=\"M 0 0 \nL -2 0 \n\" style=\"stroke: #ffffff; stroke-width: 0.6\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#mca119c7480\" x=\"79.601\" y=\"184.555452\" style=\"fill: #ffffff; stroke: #ffffff; stroke-width: 0.6\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_13\">\n      <g>\n       <use xlink:href=\"#mca119c7480\" x=\"79.601\" y=\"168.771375\" style=\"fill: #ffffff; stroke: #ffffff; stroke-width: 0.6\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_14\">\n      <g>\n       <use xlink:href=\"#mca119c7480\" x=\"79.601\" y=\"157.572393\" style=\"fill: #ffffff; stroke: #ffffff; stroke-width: 0.6\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_15\">\n      <g>\n       <use xlink:href=\"#mca119c7480\" x=\"79.601\" y=\"148.885789\" style=\"fill: #ffffff; stroke: #ffffff; stroke-width: 0.6\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_16\">\n      <g>\n       <use xlink:href=\"#mca119c7480\" x=\"79.601\" y=\"141.788316\" style=\"fill: #ffffff; stroke: #ffffff; stroke-width: 0.6\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_8\">\n     <g id=\"line2d_17\">\n      <g>\n       <use xlink:href=\"#mca119c7480\" x=\"79.601\" y=\"135.787488\" style=\"fill: #ffffff; stroke: #ffffff; stroke-width: 0.6\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_9\">\n     <g id=\"line2d_18\">\n      <g>\n       <use xlink:href=\"#mca119c7480\" x=\"79.601\" y=\"130.589335\" style=\"fill: #ffffff; stroke: #ffffff; stroke-width: 0.6\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_10\">\n     <g id=\"line2d_19\">\n      <g>\n       <use xlink:href=\"#mca119c7480\" x=\"79.601\" y=\"126.004239\" style=\"fill: #ffffff; stroke: #ffffff; stroke-width: 0.6\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_11\">\n     <g id=\"line2d_20\">\n      <g>\n       <use xlink:href=\"#mca119c7480\" x=\"79.601\" y=\"94.919672\" style=\"fill: #ffffff; stroke: #ffffff; stroke-width: 0.6\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_12\">\n     <g id=\"line2d_21\">\n      <g>\n       <use xlink:href=\"#mca119c7480\" x=\"79.601\" y=\"79.135594\" style=\"fill: #ffffff; stroke: #ffffff; stroke-width: 0.6\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_13\">\n     <g id=\"line2d_22\">\n      <g>\n       <use xlink:href=\"#mca119c7480\" x=\"79.601\" y=\"67.936613\" style=\"fill: #ffffff; stroke: #ffffff; stroke-width: 0.6\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_14\">\n     <g id=\"line2d_23\">\n      <g>\n       <use xlink:href=\"#mca119c7480\" x=\"79.601\" y=\"59.250008\" style=\"fill: #ffffff; stroke: #ffffff; stroke-width: 0.6\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_15\">\n     <g id=\"line2d_24\">\n      <g>\n       <use xlink:href=\"#mca119c7480\" x=\"79.601\" y=\"52.152535\" style=\"fill: #ffffff; stroke: #ffffff; stroke-width: 0.6\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_16\">\n     <g id=\"line2d_25\">\n      <g>\n       <use xlink:href=\"#mca119c7480\" x=\"79.601\" y=\"46.151708\" style=\"fill: #ffffff; stroke: #ffffff; stroke-width: 0.6\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_17\">\n     <g id=\"line2d_26\">\n      <g>\n       <use xlink:href=\"#mca119c7480\" x=\"79.601\" y=\"40.953554\" style=\"fill: #ffffff; stroke: #ffffff; stroke-width: 0.6\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_18\">\n     <g id=\"line2d_27\">\n      <g>\n       <use xlink:href=\"#mca119c7480\" x=\"79.601\" y=\"36.368458\" style=\"fill: #ffffff; stroke: #ffffff; stroke-width: 0.6\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_12\">\n     <!-- loss -->\n     <g style=\"fill: #ffffff\" transform=\"translate(19.3575 121.488864)rotate(-90)scale(0.16 -0.16)\">\n      <defs>\n       <path id=\"DejaVuSans-6c\" d=\"M 603 4863 \nL 1178 4863 \nL 1178 0 \nL 603 0 \nL 603 4863 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-6f\" d=\"M 1959 3097 \nQ 1497 3097 1228 2736 \nQ 959 2375 959 1747 \nQ 959 1119 1226 758 \nQ 1494 397 1959 397 \nQ 2419 397 2687 759 \nQ 2956 1122 2956 1747 \nQ 2956 2369 2687 2733 \nQ 2419 3097 1959 3097 \nz\nM 1959 3584 \nQ 2709 3584 3137 3096 \nQ 3566 2609 3566 1747 \nQ 3566 888 3137 398 \nQ 2709 -91 1959 -91 \nQ 1206 -91 779 398 \nQ 353 888 353 1747 \nQ 353 2609 779 3096 \nQ 1206 3584 1959 3584 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-73\" d=\"M 2834 3397 \nL 2834 2853 \nQ 2591 2978 2328 3040 \nQ 2066 3103 1784 3103 \nQ 1356 3103 1142 2972 \nQ 928 2841 928 2578 \nQ 928 2378 1081 2264 \nQ 1234 2150 1697 2047 \nL 1894 2003 \nQ 2506 1872 2764 1633 \nQ 3022 1394 3022 966 \nQ 3022 478 2636 193 \nQ 2250 -91 1575 -91 \nQ 1294 -91 989 -36 \nQ 684 19 347 128 \nL 347 722 \nQ 666 556 975 473 \nQ 1284 391 1588 391 \nQ 1994 391 2212 530 \nQ 2431 669 2431 922 \nQ 2431 1156 2273 1281 \nQ 2116 1406 1581 1522 \nL 1381 1569 \nQ 847 1681 609 1914 \nQ 372 2147 372 2553 \nQ 372 3047 722 3315 \nQ 1072 3584 1716 3584 \nQ 2034 3584 2315 3537 \nQ 2597 3491 2834 3397 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-6c\"/>\n      <use xlink:href=\"#DejaVuSans-6f\" x=\"27.783203\"/>\n      <use xlink:href=\"#DejaVuSans-73\" x=\"88.964844\"/>\n      <use xlink:href=\"#DejaVuSans-73\" x=\"141.064453\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_28\">\n    <path d=\"M 94.819182 16.185124 \nL 95.581998 62.448389 \nL 96.344814 75.450665 \nL 97.10763 85.061156 \nL 97.870446 90.695454 \nL 98.633262 97.750418 \nL 100.158895 109.534544 \nL 101.684527 119.024155 \nL 103.972975 127.432683 \nL 104.735792 129.014568 \nL 105.498608 131.720924 \nL 106.261424 133.951765 \nL 107.02424 133.898292 \nL 107.787056 135.604016 \nL 108.549872 137.006845 \nL 109.312688 137.688046 \nL 110.075504 137.493823 \nL 110.838321 139.642641 \nL 111.601137 138.977992 \nL 112.363953 139.417061 \nL 113.126769 139.692625 \nL 113.889585 140.614061 \nL 114.652401 140.43792 \nL 115.415217 141.036899 \nL 116.178033 141.043896 \nL 117.703666 141.636283 \nL 118.466482 142.241207 \nL 119.229298 143.409893 \nL 119.992114 142.966203 \nL 120.75493 144.047978 \nL 121.517746 143.88651 \nL 122.280563 143.437847 \nL 123.043379 144.076551 \nL 123.806195 143.793851 \nL 124.569011 144.417299 \nL 125.331827 145.8097 \nL 126.094643 145.678458 \nL 126.857459 144.324383 \nL 127.620275 146.111968 \nL 128.383092 146.815583 \nL 129.145908 147.049792 \nL 130.67154 145.824673 \nL 131.434356 147.676781 \nL 132.197172 147.569326 \nL 132.959988 148.935051 \nL 133.722805 148.649539 \nL 134.485621 148.149983 \nL 135.248437 148.135883 \nL 136.011253 148.359986 \nL 136.774069 147.643258 \nL 137.536885 148.683018 \nL 138.299701 150.561654 \nL 139.062517 149.858301 \nL 139.825334 149.886595 \nL 140.58815 152.245778 \nL 141.350966 150.381139 \nL 142.113782 151.321402 \nL 142.876598 150.5602 \nL 143.639414 150.445405 \nL 144.40223 151.751267 \nL 145.165046 152.705089 \nL 145.927863 153.438047 \nL 146.690679 152.756591 \nL 147.453495 152.977842 \nL 148.216311 154.651474 \nL 148.979127 154.77949 \nL 149.741943 154.374483 \nL 150.504759 156.432346 \nL 151.267576 156.693162 \nL 152.030392 153.534544 \nL 152.793208 156.185789 \nL 153.556024 156.986767 \nL 155.081656 157.229237 \nL 155.844472 158.588524 \nL 156.607288 157.905657 \nL 157.370105 158.047519 \nL 158.132921 157.570368 \nL 159.658553 158.845253 \nL 160.421369 158.019678 \nL 161.184185 157.743658 \nL 161.947001 159.915796 \nL 162.709817 157.777753 \nL 163.472634 158.839953 \nL 164.23545 160.428824 \nL 164.998266 160.496715 \nL 165.761082 163.164487 \nL 166.523898 162.929472 \nL 167.286714 161.976934 \nL 168.04953 162.460474 \nL 168.812347 162.10157 \nL 169.575163 160.744272 \nL 170.337979 162.168996 \nL 171.100795 162.7768 \nL 171.863611 163.025036 \nL 172.626427 163.625076 \nL 173.389243 164.019294 \nL 174.152059 163.623089 \nL 174.914876 164.257976 \nL 175.677692 164.405712 \nL 176.440508 166.838926 \nL 177.203324 165.109593 \nL 178.728956 165.656929 \nL 179.491772 167.389158 \nL 180.254589 165.014366 \nL 181.017405 166.622699 \nL 181.780221 165.89351 \nL 182.543037 165.37673 \nL 183.305853 167.633827 \nL 184.831485 168.529667 \nL 185.594301 166.241317 \nL 186.357118 167.098977 \nL 187.119934 168.404605 \nL 187.88275 166.525993 \nL 188.645566 169.396789 \nL 189.408382 170.161853 \nL 190.171198 168.789718 \nL 190.934014 170.243306 \nL 191.69683 168.275967 \nL 192.459647 172.298733 \nL 193.222463 169.232343 \nL 193.985279 169.037619 \nL 194.748095 172.079295 \nL 195.510911 171.565761 \nL 196.273727 170.119338 \nL 197.036543 169.752637 \nL 197.79936 170.005661 \nL 198.562176 171.2846 \nL 199.324992 170.218293 \nL 200.087808 172.771281 \nL 200.850624 171.351071 \nL 201.61344 172.036601 \nL 202.376256 171.690654 \nL 203.139072 170.941052 \nL 203.901889 169.972162 \nL 204.664705 172.698036 \nL 205.427521 174.901675 \nL 206.190337 172.919668 \nL 207.715969 173.101468 \nL 208.478785 173.81259 \nL 209.241602 174.893313 \nL 210.004418 172.759037 \nL 210.767234 173.806505 \nL 211.53005 173.520252 \nL 212.292866 174.895771 \nL 213.055682 174.800117 \nL 213.818498 173.705678 \nL 214.581314 173.071168 \nL 215.344131 174.488933 \nL 216.106947 174.10135 \nL 216.869763 173.975447 \nL 217.632579 176.380763 \nL 218.395395 172.095363 \nL 219.158211 174.689422 \nL 219.921027 175.874744 \nL 220.683843 174.795182 \nL 221.44666 175.049482 \nL 222.209476 176.435033 \nL 222.972292 176.287452 \nL 223.735108 176.301729 \nL 224.497924 176.558187 \nL 225.26074 176.430702 \nL 226.023556 178.887213 \nL 227.549189 176.64112 \nL 228.312005 177.664122 \nL 229.074821 178.166791 \nL 229.837637 178.342043 \nL 230.600453 176.750404 \nL 231.363269 176.009581 \nL 232.126085 180.299476 \nL 232.888902 176.22947 \nL 233.651718 177.578728 \nL 234.414534 177.363005 \nL 235.17735 177.733774 \nL 235.940166 176.605339 \nL 236.702982 178.83455 \nL 237.465798 180.069739 \nL 238.228614 178.846177 \nL 238.991431 180.647461 \nL 239.754247 179.270605 \nL 240.517063 177.26873 \nL 241.279879 182.540259 \nL 242.042695 179.382806 \nL 242.805511 179.04903 \nL 243.568327 180.571154 \nL 244.331144 177.847476 \nL 245.09396 180.631489 \nL 245.856776 181.296649 \nL 246.619592 179.629831 \nL 247.382408 180.804978 \nL 248.145224 180.951882 \nL 248.90804 177.836239 \nL 250.433673 181.74567 \nL 251.196489 179.974734 \nL 251.959305 183.782908 \nL 252.722121 179.243672 \nL 253.484937 182.67907 \nL 254.247753 182.114259 \nL 255.010569 180.789315 \nL 255.773386 180.715167 \nL 256.536202 182.218671 \nL 257.299018 182.940861 \nL 258.061834 180.936706 \nL 258.82465 183.081239 \nL 259.587466 181.581454 \nL 261.113098 183.151712 \nL 261.875915 182.794763 \nL 262.638731 183.613431 \nL 263.401547 183.840924 \nL 264.927179 180.632741 \nL 265.689995 184.484162 \nL 266.452811 185.684463 \nL 267.215627 184.521624 \nL 267.978444 181.811858 \nL 268.74126 185.005901 \nL 269.504076 183.147068 \nL 270.266892 183.480732 \nL 271.029708 185.997368 \nL 271.792524 181.690823 \nL 272.55534 185.107827 \nL 273.318157 183.319611 \nL 274.080973 183.229162 \nL 274.843789 185.516252 \nL 275.606605 184.370114 \nL 276.369421 185.239602 \nL 277.132237 184.43875 \nL 277.895053 185.780992 \nL 278.657869 184.378086 \nL 279.420686 182.260261 \nL 280.183502 186.108737 \nL 280.946318 183.762302 \nL 281.709134 187.040573 \nL 282.47195 186.919198 \nL 283.234766 183.582507 \nL 283.997582 183.753085 \nL 284.760398 185.845941 \nL 285.523215 185.666567 \nL 286.286031 183.861735 \nL 287.048847 185.199618 \nL 287.811663 184.611994 \nL 288.574479 185.988701 \nL 289.337295 185.625286 \nL 290.100111 184.040388 \nL 290.862928 188.327655 \nL 291.625744 185.835014 \nL 292.38856 186.590863 \nL 293.151376 186.123428 \nL 293.914192 185.133301 \nL 294.677008 185.523692 \nL 295.439824 188.078084 \nL 296.20264 185.574966 \nL 296.965457 184.825237 \nL 297.728273 185.81727 \nL 298.491089 186.36691 \nL 299.253905 187.454311 \nL 300.016721 185.353749 \nL 300.779537 188.535312 \nL 301.542353 186.085166 \nL 302.30517 188.9518 \nL 303.067986 187.797961 \nL 303.830802 188.190725 \nL 304.593618 185.123176 \nL 305.356434 186.838619 \nL 306.11925 187.748509 \nL 306.882066 185.836753 \nL 307.644882 188.417844 \nL 308.407699 187.948099 \nL 309.170515 186.714862 \nL 309.933331 188.30354 \nL 310.696147 189.103604 \nL 311.458963 187.825979 \nL 312.221779 187.409191 \nL 312.984595 188.550683 \nL 313.747411 187.329336 \nL 314.510228 189.015647 \nL 315.273044 186.680264 \nL 316.03586 190.480271 \nL 316.798676 190.753241 \nL 317.561492 188.240375 \nL 318.324308 187.835042 \nL 319.087124 188.560398 \nL 319.849941 187.731456 \nL 320.612757 188.654006 \nL 321.375573 189.02835 \nL 322.138389 187.378317 \nL 322.901205 191.274407 \nL 324.426837 188.475467 \nL 325.189653 188.104804 \nL 325.95247 186.8368 \nL 326.715286 186.252185 \nL 327.478102 189.595058 \nL 328.240918 189.596552 \nL 329.003734 188.567459 \nL 329.76655 188.961088 \nL 330.529366 188.565763 \nL 331.292183 189.506384 \nL 332.054999 188.855483 \nL 332.817815 190.468183 \nL 333.580631 189.122748 \nL 334.343447 191.376984 \nL 335.106263 190.056065 \nL 335.869079 191.009109 \nL 336.631895 189.80827 \nL 337.394712 189.404687 \nL 338.157528 191.573871 \nL 338.920344 190.736244 \nL 339.68316 192.066573 \nL 341.208792 189.728115 \nL 341.971608 192.788679 \nL 342.734424 191.900898 \nL 343.497241 191.779443 \nL 344.260057 191.233196 \nL 345.022873 191.951799 \nL 345.785689 189.865618 \nL 346.548505 191.757473 \nL 347.311321 190.157634 \nL 348.074137 192.398768 \nL 348.836954 191.987006 \nL 349.59977 189.385194 \nL 351.125402 192.866788 \nL 351.888218 190.390731 \nL 353.41385 192.101229 \nL 354.176666 188.863104 \nL 354.939483 190.954796 \nL 356.465115 192.874324 \nL 357.227931 190.307856 \nL 357.990747 191.176654 \nL 358.753563 190.911174 \nL 359.516379 192.092538 \nL 360.279195 191.872904 \nL 361.042012 192.182031 \nL 361.804828 193.75527 \nL 362.567644 190.693375 \nL 363.33046 193.384684 \nL 364.093276 191.765365 \nL 364.856092 192.33481 \nL 365.618908 191.155932 \nL 366.381725 193.151668 \nL 367.144541 192.501581 \nL 367.907357 192.002403 \nL 369.432989 193.087008 \nL 370.195805 192.625028 \nL 370.958621 190.868896 \nL 371.721437 192.687491 \nL 372.484254 193.254234 \nL 373.24707 191.605513 \nL 374.009886 193.722676 \nL 374.772702 192.083083 \nL 375.535518 191.019723 \nL 377.06115 194.086982 \nL 377.823967 193.645401 \nL 378.586783 192.016488 \nL 379.349599 193.625927 \nL 380.112415 191.91994 \nL 380.875231 192.369031 \nL 381.638047 192.460247 \nL 382.400863 194.15134 \nL 383.163679 191.148759 \nL 383.926496 192.964825 \nL 384.689312 192.184703 \nL 385.452128 192.058246 \nL 386.214944 193.303832 \nL 386.97776 192.631327 \nL 387.740576 193.005911 \nL 388.503392 193.867536 \nL 389.266208 192.776935 \nL 390.029025 192.601702 \nL 390.791841 193.479038 \nL 391.554657 192.803405 \nL 393.843105 195.525531 \nL 394.605921 192.559063 \nL 395.368738 194.04523 \nL 396.131554 195.887603 \nL 396.89437 195.195489 \nL 397.657186 190.354051 \nL 398.420002 193.607249 \nL 399.182818 194.060347 \nL 399.182818 194.060347 \n\" clip-path=\"url(#pbed146e419)\" style=\"fill: none; stroke: #8dd3c7; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_29\">\n    <path d=\"M 94.819182 79.576913 \nL 96.344814 86.586933 \nL 97.870446 96.328491 \nL 99.396079 108.477594 \nL 100.158895 113.795971 \nL 101.684527 122.318889 \nL 102.447343 126.041405 \nL 103.210159 127.331862 \nL 103.972975 129.685785 \nL 105.498608 132.670859 \nL 106.261424 132.90235 \nL 107.02424 133.271443 \nL 107.787056 133.412788 \nL 108.549872 134.114142 \nL 109.312688 132.853818 \nL 110.075504 133.71746 \nL 110.838321 133.951042 \nL 111.601137 133.314374 \nL 112.363953 133.703174 \nL 113.126769 133.571902 \nL 113.889585 134.609228 \nL 114.652401 133.677509 \nL 115.415217 133.852343 \nL 116.178033 133.57292 \nL 116.94085 134.675528 \nL 117.703666 134.051037 \nL 118.466482 135.31544 \nL 119.229298 134.355989 \nL 119.992114 133.990045 \nL 120.75493 133.896674 \nL 121.517746 134.126667 \nL 122.280563 135.571418 \nL 123.043379 135.652787 \nL 123.806195 134.140867 \nL 124.569011 135.057727 \nL 125.331827 135.747173 \nL 126.094643 135.492023 \nL 126.857459 135.670588 \nL 127.620275 135.573746 \nL 128.383092 136.831183 \nL 129.145908 135.851449 \nL 129.908724 136.436822 \nL 130.67154 137.996328 \nL 131.434356 136.838665 \nL 132.197172 137.980277 \nL 132.959988 136.840317 \nL 133.722805 135.966363 \nL 134.485621 136.147574 \nL 135.248437 138.547249 \nL 136.011253 138.282373 \nL 136.774069 137.604931 \nL 137.536885 137.991531 \nL 138.299701 139.957035 \nL 139.062517 137.973654 \nL 139.825334 137.775005 \nL 141.350966 138.826441 \nL 142.113782 140.026736 \nL 142.876598 138.340181 \nL 143.639414 137.508333 \nL 144.40223 138.942318 \nL 145.165046 138.628426 \nL 145.927863 139.23678 \nL 146.690679 139.369919 \nL 147.453495 140.822161 \nL 148.216311 141.420163 \nL 148.979127 143.241564 \nL 149.741943 145.77793 \nL 151.267576 140.951036 \nL 152.030392 140.475549 \nL 152.793208 143.720316 \nL 153.556024 141.529637 \nL 154.31884 145.462459 \nL 155.081656 142.643054 \nL 155.844472 145.006151 \nL 156.607288 143.185836 \nL 157.370105 145.338957 \nL 158.132921 141.735375 \nL 158.895737 144.892973 \nL 159.658553 143.800878 \nL 160.421369 149.916494 \nL 161.184185 142.856494 \nL 161.947001 143.967877 \nL 162.709817 140.152838 \nL 163.472634 146.522732 \nL 164.23545 150.522132 \nL 164.998266 150.07555 \nL 165.761082 148.163692 \nL 166.523898 145.710638 \nL 167.286714 150.80833 \nL 168.04953 146.548131 \nL 168.812347 146.12846 \nL 169.575163 147.679739 \nL 170.337979 150.70849 \nL 171.100795 148.424167 \nL 172.626427 150.61323 \nL 173.389243 149.805125 \nL 174.152059 147.113455 \nL 174.914876 153.660966 \nL 175.677692 151.960304 \nL 176.440508 151.58254 \nL 177.203324 147.911349 \nL 177.96614 150.669068 \nL 178.728956 156.02946 \nL 179.491772 152.270676 \nL 180.254589 151.19802 \nL 181.017405 151.255656 \nL 181.780221 150.891064 \nL 182.543037 152.228455 \nL 183.305853 149.579563 \nL 184.068669 149.41423 \nL 184.831485 151.520582 \nL 185.594301 154.879625 \nL 186.357118 155.517174 \nL 187.119934 145.975794 \nL 187.88275 150.594618 \nL 189.408382 156.736252 \nL 190.171198 154.074613 \nL 190.934014 149.755533 \nL 191.69683 153.175511 \nL 192.459647 155.019164 \nL 193.222463 155.968263 \nL 193.985279 149.511923 \nL 194.748095 152.100692 \nL 195.510911 157.102056 \nL 196.273727 158.237711 \nL 197.036543 155.147709 \nL 197.79936 153.967588 \nL 198.562176 155.585847 \nL 199.324992 153.740837 \nL 200.087808 155.291383 \nL 200.850624 156.51286 \nL 201.61344 158.142998 \nL 202.376256 160.091923 \nL 203.139072 155.700417 \nL 203.901889 156.926458 \nL 204.664705 155.48284 \nL 205.427521 158.220758 \nL 206.190337 159.081645 \nL 206.953153 160.234488 \nL 207.715969 159.429034 \nL 208.478785 160.566408 \nL 209.241602 158.633545 \nL 210.004418 156.362752 \nL 210.767234 157.911662 \nL 211.53005 153.383099 \nL 212.292866 158.773262 \nL 213.055682 160.893285 \nL 213.818498 162.016468 \nL 214.581314 160.668693 \nL 215.344131 158.919236 \nL 216.106947 155.112892 \nL 217.632579 159.624869 \nL 218.395395 154.659275 \nL 219.158211 161.798073 \nL 219.921027 162.137977 \nL 220.683843 155.560227 \nL 221.44666 157.838061 \nL 222.209476 164.334789 \nL 222.972292 159.017978 \nL 223.735108 165.184818 \nL 224.497924 160.283608 \nL 225.26074 164.406935 \nL 226.023556 159.473162 \nL 226.786373 159.574943 \nL 227.549189 161.006395 \nL 228.312005 162.109912 \nL 229.837637 158.710661 \nL 230.600453 154.776163 \nL 231.363269 166.186084 \nL 232.126085 158.184541 \nL 232.888902 157.818719 \nL 233.651718 162.328991 \nL 234.414534 163.966166 \nL 235.17735 154.751469 \nL 235.940166 160.926931 \nL 236.702982 153.241346 \nL 237.465798 156.787923 \nL 238.228614 161.792201 \nL 238.991431 155.470153 \nL 239.754247 164.972998 \nL 240.517063 160.545165 \nL 241.279879 164.196402 \nL 242.042695 161.846338 \nL 242.805511 160.361004 \nL 243.568327 162.049656 \nL 244.331144 158.206336 \nL 245.09396 160.726918 \nL 245.856776 156.100778 \nL 246.619592 159.676428 \nL 247.382408 161.564989 \nL 248.145224 157.52674 \nL 248.90804 163.478748 \nL 249.670856 166.662324 \nL 250.433673 163.733996 \nL 251.196489 164.727766 \nL 251.959305 163.877187 \nL 252.722121 156.838069 \nL 253.484937 158.917723 \nL 254.247753 166.767781 \nL 255.010569 159.355615 \nL 255.773386 160.264997 \nL 256.536202 160.241107 \nL 257.299018 151.360913 \nL 258.061834 160.068553 \nL 258.82465 164.190462 \nL 259.587466 157.523499 \nL 260.350282 157.910598 \nL 261.113098 162.343696 \nL 261.875915 158.490634 \nL 262.638731 157.753618 \nL 263.401547 157.465181 \nL 264.164363 155.245627 \nL 264.927179 160.041492 \nL 265.689995 152.879396 \nL 266.452811 160.606878 \nL 267.215627 159.032625 \nL 267.978444 155.95479 \nL 268.74126 162.663248 \nL 269.504076 159.29176 \nL 270.266892 160.086431 \nL 271.029708 163.654874 \nL 271.792524 159.229008 \nL 272.55534 159.423567 \nL 273.318157 156.719549 \nL 274.080973 158.195861 \nL 274.843789 157.855334 \nL 275.606605 159.382591 \nL 276.369421 163.315821 \nL 277.132237 161.491057 \nL 277.895053 157.21665 \nL 278.657869 159.539794 \nL 279.420686 159.9127 \nL 280.183502 156.329683 \nL 280.946318 154.228508 \nL 281.709134 162.310867 \nL 282.47195 157.45401 \nL 283.234766 161.219087 \nL 283.997582 160.023055 \nL 284.760398 161.272811 \nL 285.523215 155.866673 \nL 286.286031 162.768579 \nL 287.048847 159.851085 \nL 287.811663 164.169703 \nL 288.574479 153.450728 \nL 289.337295 160.199382 \nL 290.100111 163.691649 \nL 290.862928 166.477842 \nL 291.625744 159.081513 \nL 292.38856 159.437682 \nL 293.151376 167.983803 \nL 293.914192 161.297551 \nL 295.439824 153.156057 \nL 296.20264 161.031275 \nL 296.965457 156.181391 \nL 297.728273 155.374773 \nL 298.491089 157.774896 \nL 299.253905 164.701014 \nL 300.016721 155.803565 \nL 300.779537 154.870123 \nL 301.542353 164.798174 \nL 302.30517 156.605937 \nL 303.067986 155.904827 \nL 303.830802 162.649594 \nL 304.593618 158.261495 \nL 305.356434 163.135785 \nL 306.11925 160.703171 \nL 306.882066 157.411152 \nL 307.644882 158.103351 \nL 309.170515 159.870583 \nL 309.933331 155.396174 \nL 310.696147 159.840362 \nL 311.458963 157.259079 \nL 312.221779 157.750391 \nL 312.984595 155.684091 \nL 313.747411 153.977497 \nL 314.510228 153.7995 \nL 315.273044 166.0755 \nL 316.03586 154.365975 \nL 316.798676 162.802676 \nL 317.561492 161.309609 \nL 318.324308 161.66992 \nL 319.087124 158.374528 \nL 319.849941 153.371322 \nL 320.612757 162.573291 \nL 321.375573 152.689693 \nL 322.138389 157.134092 \nL 322.901205 165.28091 \nL 323.664021 158.775981 \nL 324.426837 154.662726 \nL 325.189653 155.503173 \nL 325.95247 149.564284 \nL 326.715286 150.132628 \nL 328.240918 161.359686 \nL 329.003734 156.385136 \nL 329.76655 156.813699 \nL 330.529366 150.111688 \nL 331.292183 156.8403 \nL 332.054999 154.68211 \nL 332.817815 153.997822 \nL 333.580631 160.858574 \nL 334.343447 157.114434 \nL 335.106263 155.102173 \nL 335.869079 158.541704 \nL 336.631895 155.435236 \nL 337.394712 160.779888 \nL 338.157528 160.594216 \nL 338.920344 159.675728 \nL 339.68316 160.668559 \nL 341.208792 152.931246 \nL 343.497241 159.827336 \nL 344.260057 154.287358 \nL 345.022873 153.530367 \nL 345.785689 154.371594 \nL 346.548505 153.073358 \nL 347.311321 159.6744 \nL 348.074137 159.831386 \nL 348.836954 160.811715 \nL 349.59977 157.879404 \nL 350.362586 156.442435 \nL 351.125402 157.471338 \nL 351.888218 153.896668 \nL 352.651034 155.455907 \nL 353.41385 164.267379 \nL 354.176666 150.239407 \nL 354.939483 154.481087 \nL 355.702299 156.589085 \nL 356.465115 152.515368 \nL 357.227931 156.403092 \nL 357.990747 155.271683 \nL 358.753563 158.721813 \nL 359.516379 154.996442 \nL 360.279195 148.682022 \nL 361.042012 161.400265 \nL 361.804828 150.366006 \nL 362.567644 154.878495 \nL 363.33046 157.349389 \nL 364.093276 153.755027 \nL 364.856092 155.821741 \nL 365.618908 154.255686 \nL 366.381725 150.411611 \nL 367.144541 150.540376 \nL 367.907357 155.946656 \nL 368.670173 154.656011 \nL 369.432989 151.704856 \nL 370.195805 145.90968 \nL 370.958621 158.072865 \nL 371.721437 162.16845 \nL 372.484254 156.101818 \nL 373.24707 155.136353 \nL 374.009886 156.342007 \nL 374.772702 153.624276 \nL 375.535518 159.951394 \nL 376.298334 150.719261 \nL 377.06115 157.015052 \nL 378.586783 157.542155 \nL 379.349599 156.181745 \nL 380.112415 152.821252 \nL 380.875231 151.652359 \nL 381.638047 154.918656 \nL 383.163679 160.279643 \nL 383.926496 156.927071 \nL 384.689312 154.369274 \nL 385.452128 153.642471 \nL 386.97776 160.362796 \nL 387.740576 157.058238 \nL 388.503392 154.276767 \nL 389.266208 153.321933 \nL 390.029025 155.356425 \nL 390.791841 150.26037 \nL 391.554657 155.542824 \nL 392.317473 157.337012 \nL 393.080289 156.218126 \nL 393.843105 160.784841 \nL 394.605921 155.344224 \nL 395.368738 153.325755 \nL 396.131554 160.821728 \nL 396.89437 151.936642 \nL 397.657186 151.892895 \nL 398.420002 158.015688 \nL 399.182818 152.537209 \nL 399.182818 152.537209 \n\" clip-path=\"url(#pbed146e419)\" style=\"fill: none; stroke: #feffb3; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 79.601 204.872727 \nL 79.601 7.2 \n\" style=\"fill: none; stroke: #ffffff; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 414.401 204.872727 \nL 414.401 7.2 \n\" style=\"fill: none; stroke: #ffffff; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 79.601 204.872727 \nL 414.401 204.872727 \n\" style=\"fill: none; stroke: #ffffff; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 79.601 7.2 \nL 414.401 7.2 \n\" style=\"fill: none; stroke: #ffffff; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n  </g>\n  <g id=\"axes_2\">\n   <g id=\"patch_7\">\n    <path d=\"M 79.601 442.08 \nL 414.401 442.08 \nL 414.401 244.407273 \nL 79.601 244.407273 \nz\n\"/>\n   </g>\n   <g id=\"matplotlib.axis_3\">\n    <g id=\"xtick_10\">\n     <g id=\"line2d_30\">\n      <g>\n       <use xlink:href=\"#mf7db47d992\" x=\"94.819182\" y=\"442.08\" style=\"fill: #ffffff; stroke: #ffffff; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 0 -->\n      <g style=\"fill: #ffffff\" transform=\"translate(91.637932 456.678437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_11\">\n     <g id=\"line2d_31\">\n      <g>\n       <use xlink:href=\"#mf7db47d992\" x=\"132.959988\" y=\"442.08\" style=\"fill: #ffffff; stroke: #ffffff; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 50 -->\n      <g style=\"fill: #ffffff\" transform=\"translate(126.597488 456.678437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-35\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_12\">\n     <g id=\"line2d_32\">\n      <g>\n       <use xlink:href=\"#mf7db47d992\" x=\"171.100795\" y=\"442.08\" style=\"fill: #ffffff; stroke: #ffffff; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_15\">\n      <!-- 100 -->\n      <g style=\"fill: #ffffff\" transform=\"translate(161.557045 456.678437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_13\">\n     <g id=\"line2d_33\">\n      <g>\n       <use xlink:href=\"#mf7db47d992\" x=\"209.241602\" y=\"442.08\" style=\"fill: #ffffff; stroke: #ffffff; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_16\">\n      <!-- 150 -->\n      <g style=\"fill: #ffffff\" transform=\"translate(199.697852 456.678437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-35\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_14\">\n     <g id=\"line2d_34\">\n      <g>\n       <use xlink:href=\"#mf7db47d992\" x=\"247.382408\" y=\"442.08\" style=\"fill: #ffffff; stroke: #ffffff; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_17\">\n      <!-- 200 -->\n      <g style=\"fill: #ffffff\" transform=\"translate(237.838658 456.678437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_15\">\n     <g id=\"line2d_35\">\n      <g>\n       <use xlink:href=\"#mf7db47d992\" x=\"285.523215\" y=\"442.08\" style=\"fill: #ffffff; stroke: #ffffff; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_18\">\n      <!-- 250 -->\n      <g style=\"fill: #ffffff\" transform=\"translate(275.979465 456.678437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use xlink:href=\"#DejaVuSans-35\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_16\">\n     <g id=\"line2d_36\">\n      <g>\n       <use xlink:href=\"#mf7db47d992\" x=\"323.664021\" y=\"442.08\" style=\"fill: #ffffff; stroke: #ffffff; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_19\">\n      <!-- 300 -->\n      <g style=\"fill: #ffffff\" transform=\"translate(314.120271 456.678437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-33\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_17\">\n     <g id=\"line2d_37\">\n      <g>\n       <use xlink:href=\"#mf7db47d992\" x=\"361.804828\" y=\"442.08\" style=\"fill: #ffffff; stroke: #ffffff; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_20\">\n      <!-- 350 -->\n      <g style=\"fill: #ffffff\" transform=\"translate(352.261078 456.678437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-33\"/>\n       <use xlink:href=\"#DejaVuSans-35\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_18\">\n     <g id=\"line2d_38\">\n      <g>\n       <use xlink:href=\"#mf7db47d992\" x=\"399.945634\" y=\"442.08\" style=\"fill: #ffffff; stroke: #ffffff; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_21\">\n      <!-- 400 -->\n      <g style=\"fill: #ffffff\" transform=\"translate(390.401884 456.678437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-34\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_4\">\n    <g id=\"ytick_19\">\n     <g id=\"line2d_39\">\n      <g>\n       <use xlink:href=\"#m6ebf0f4420\" x=\"79.601\" y=\"442.08\" style=\"fill: #ffffff; stroke: #ffffff; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_22\">\n      <!-- 0.0 -->\n      <g style=\"fill: #ffffff\" transform=\"translate(56.697875 445.879219)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-2e\" d=\"M 684 794 \nL 1344 794 \nL 1344 0 \nL 684 0 \nL 684 794 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_20\">\n     <g id=\"line2d_40\">\n      <g>\n       <use xlink:href=\"#m6ebf0f4420\" x=\"79.601\" y=\"402.545455\" style=\"fill: #ffffff; stroke: #ffffff; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_23\">\n      <!-- 0.2 -->\n      <g style=\"fill: #ffffff\" transform=\"translate(56.697875 406.344673)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-32\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_21\">\n     <g id=\"line2d_41\">\n      <g>\n       <use xlink:href=\"#m6ebf0f4420\" x=\"79.601\" y=\"363.010909\" style=\"fill: #ffffff; stroke: #ffffff; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_24\">\n      <!-- 0.4 -->\n      <g style=\"fill: #ffffff\" transform=\"translate(56.697875 366.810128)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-34\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_22\">\n     <g id=\"line2d_42\">\n      <g>\n       <use xlink:href=\"#m6ebf0f4420\" x=\"79.601\" y=\"323.476364\" style=\"fill: #ffffff; stroke: #ffffff; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_25\">\n      <!-- 0.6 -->\n      <g style=\"fill: #ffffff\" transform=\"translate(56.697875 327.275582)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-36\" d=\"M 2113 2584 \nQ 1688 2584 1439 2293 \nQ 1191 2003 1191 1497 \nQ 1191 994 1439 701 \nQ 1688 409 2113 409 \nQ 2538 409 2786 701 \nQ 3034 994 3034 1497 \nQ 3034 2003 2786 2293 \nQ 2538 2584 2113 2584 \nz\nM 3366 4563 \nL 3366 3988 \nQ 3128 4100 2886 4159 \nQ 2644 4219 2406 4219 \nQ 1781 4219 1451 3797 \nQ 1122 3375 1075 2522 \nQ 1259 2794 1537 2939 \nQ 1816 3084 2150 3084 \nQ 2853 3084 3261 2657 \nQ 3669 2231 3669 1497 \nQ 3669 778 3244 343 \nQ 2819 -91 2113 -91 \nQ 1303 -91 875 529 \nQ 447 1150 447 2328 \nQ 447 3434 972 4092 \nQ 1497 4750 2381 4750 \nQ 2619 4750 2861 4703 \nQ 3103 4656 3366 4563 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-36\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_23\">\n     <g id=\"line2d_43\">\n      <g>\n       <use xlink:href=\"#m6ebf0f4420\" x=\"79.601\" y=\"283.941818\" style=\"fill: #ffffff; stroke: #ffffff; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_26\">\n      <!-- 0.8 -->\n      <g style=\"fill: #ffffff\" transform=\"translate(56.697875 287.741037)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-38\" d=\"M 2034 2216 \nQ 1584 2216 1326 1975 \nQ 1069 1734 1069 1313 \nQ 1069 891 1326 650 \nQ 1584 409 2034 409 \nQ 2484 409 2743 651 \nQ 3003 894 3003 1313 \nQ 3003 1734 2745 1975 \nQ 2488 2216 2034 2216 \nz\nM 1403 2484 \nQ 997 2584 770 2862 \nQ 544 3141 544 3541 \nQ 544 4100 942 4425 \nQ 1341 4750 2034 4750 \nQ 2731 4750 3128 4425 \nQ 3525 4100 3525 3541 \nQ 3525 3141 3298 2862 \nQ 3072 2584 2669 2484 \nQ 3125 2378 3379 2068 \nQ 3634 1759 3634 1313 \nQ 3634 634 3220 271 \nQ 2806 -91 2034 -91 \nQ 1263 -91 848 271 \nQ 434 634 434 1313 \nQ 434 1759 690 2068 \nQ 947 2378 1403 2484 \nz\nM 1172 3481 \nQ 1172 3119 1398 2916 \nQ 1625 2713 2034 2713 \nQ 2441 2713 2670 2916 \nQ 2900 3119 2900 3481 \nQ 2900 3844 2670 4047 \nQ 2441 4250 2034 4250 \nQ 1625 4250 1398 4047 \nQ 1172 3844 1172 3481 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-38\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_24\">\n     <g id=\"line2d_44\">\n      <g>\n       <use xlink:href=\"#m6ebf0f4420\" x=\"79.601\" y=\"244.407273\" style=\"fill: #ffffff; stroke: #ffffff; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_27\">\n      <!-- 1.0 -->\n      <g style=\"fill: #ffffff\" transform=\"translate(56.697875 248.206491)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_8\">\n    <path d=\"M 79.601 442.08 \nL 79.601 244.407273 \n\" style=\"fill: none; stroke: #ffffff; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_9\">\n    <path d=\"M 414.401 442.08 \nL 414.401 244.407273 \n\" style=\"fill: none; stroke: #ffffff; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_10\">\n    <path d=\"M 79.601 442.08 \nL 414.401 442.08 \n\" style=\"fill: none; stroke: #ffffff; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_11\">\n    <path d=\"M 79.601 244.407273 \nL 414.401 244.407273 \n\" style=\"fill: none; stroke: #ffffff; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"pbed146e419\">\n   <rect x=\"79.601\" y=\"7.2\" width=\"334.8\" height=\"197.672727\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "text/plain": [
       "<Figure size 432x576 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Evaluating the model on the training and testing set\n",
    "print(\"Final Training Loss: {0:.3}\".format(history.history['val_loss'][-1]))\n",
    "print(\"Final Testing Loss: {0:.3}\".format(history.history['val_loss'][-1]))\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "# source: https://stackoverflow.com/questions/31810461/python-matplotlib-vertically-aligned-plots-in-matplotlib\n",
    "\n",
    "gs = gridspec.GridSpec(2,1)\n",
    "fig = plt.figure(figsize=(6,8))\n",
    "\n",
    "#first plot\n",
    "ax = fig.add_subplot(gs[0])\n",
    "ax.plot(history.history['loss'])\n",
    "ax.plot(history.history['val_loss'])\n",
    "ax.set_yscale('log')\n",
    "ax.set_ylabel(r'loss', size=16)\n",
    "ax.get_yaxis().set_label_coords(-0.17,0.5)\n",
    "plt.tick_params(\n",
    "    axis='x',          # changes apply to the x-axis\n",
    "    labelbottom='off') # labels along the bottom edge are off\n",
    "\n",
    "#second plot\n",
    "ax = fig.add_subplot(gs[1], sharex=ax)\n",
    "ax.plot(history.history['lr'])\n",
    "ax.set_ylabel(r'learning rate', size =16)\n",
    "ax.get_yaxis().set_label_coords(-0.17,0.5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions on unseen data\n",
    "\n",
    "model.predict(x=x_, batch_size=)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4e86dda363227cd61aa00d3be7fff4df592e7d0ccc5ab5a4cab2f3ca027a6e6b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('env_tf_and_audio')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
